{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b91ed8",
   "metadata": {},
   "source": [
    "# Explanation of model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b28cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
      "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
      "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
      "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
      "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
      "2010 marriage license application, according to court documents.\n",
      "Prosecutors said the marriages were part of an immigration scam.\n",
      "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
      "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
      "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
      "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
      "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
      "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
      "The case was referred to the Bronx District Attorney's Office by Immigration and Customs Enforcement and the Department of Homeland Security's\n",
      "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
      "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
      "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n"
     ]
    }
   ],
   "source": [
    "with open(\"sample_article.txt\", \"r\") as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d0fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# sample_app_request = {\"text\": sample_text}\n",
    "\n",
    "# with open(\"sample_app_input.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(sample_app_request, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329fe7e",
   "metadata": {},
   "source": [
    "## Normal pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a9283c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. If convicted, she faces up to four years in prison.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "result = summarizer(sample_text, max_length=130, min_length=30, do_sample=False)\n",
    "\n",
    "print(result[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76bb1c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_num_labels\": 3,\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.0,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 1024,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 4096,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 12,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 4096,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 12,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"force_bos_token_to_be_generated\": true,\n",
       "  \"forced_bos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"length_penalty\": 2.0,\n",
       "  \"max_length\": 142,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"min_length\": 56,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"prefix\": \" \",\n",
       "  \"scale_embedding\": false,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 142,\n",
       "      \"min_length\": 56,\n",
       "      \"no_repeat_ngram_size\": 3,\n",
       "      \"num_beams\": 4\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.56.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50264\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer.model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e9ba5",
   "metadata": {},
   "source": [
    "## Step by Step Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04fe750",
   "metadata": {},
   "source": [
    "### Understanding HuggingFace's Summarization Pipeline\n",
    "\n",
    "HuggingFace's pipeline API uses **`SummarizationPipeline`**, which is a `Text2TextGenerationPipeline` under the hood when `pipeline(\"summarization\", ...)` is called. \n",
    "\n",
    "#### Pipeline Execution Flow\n",
    "\n",
    "When we run `summarizer(...)`, the execution follows this chain:\n",
    "\n",
    "```\n",
    "SummarizationPipeline.__call__() \n",
    "    ‚Üì\n",
    "Text2TextGenerationPipeline.__call__() \n",
    "    ‚Üì\n",
    "Pipeline.__call__()\n",
    "```\n",
    "\n",
    "#### üîÑ Three-Step Process\n",
    "\n",
    "The `Pipeline.__call__()` method executes these functions **in order**:\n",
    "\n",
    "| Step | Function | Purpose |\n",
    "|------|----------|---------|\n",
    "| **1** | `self.preprocess(inputs, **preprocess_params)` | Prepares input data for the model |\n",
    "| **2** | `self.forward(model_inputs, **forward_params)` | Runs the model inference |\n",
    "| **3** | `self.postprocess(model_outputs, **postprocess_params)` | Processes model outputs into final format |\n",
    "\n",
    "\n",
    "**üí° Parameter Management**: The `preprocess_params`, `forward_params`, and `postprocess_params` come from `self._sanitize_parameters(**kwargs)`, which validates and formats the configuration parameters for each function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f06a3",
   "metadata": {},
   "source": [
    "#### üîç **Preprocessing**\n",
    "\n",
    "The preprocessing step is crucial in transforming raw text input into a format that the model can understand. Let's break down exactly what happens in the implementation:\n",
    "\n",
    "1. **Input Validation & Type Handling**: Ensures input is either a single string or list of strings, since different formats require their special processing strategies.\n",
    "```python\n",
    "assert isinstance(args[0], str) or isinstance(args[0], list)\n",
    "```\n",
    "\n",
    "2. **Prefix Application**: Prefixes allow you to condition your model with a task specific starter. For example, for a general purpose text to text generator, tasks can be `\"summarize: \"` or `\"translate English to French: \"`. But for BART, there is no need for a task-specific prefix.\n",
    "```python\n",
    "prefix = prefix if prefix is not None else \"\"\n",
    "```\n",
    "\n",
    "3. **Padding Strategy**: From Step 1, we can understand that input can be a single string, or a batch of strings. Prefix is applied to every string separately.\n",
    "```python\n",
    "if isinstance(args[0], list):\n",
    "    args = ([prefix + arg for arg in args[0]],)\n",
    "    padding = True\n",
    "elif isinstance(args[0], str):\n",
    "    args = (prefix + args[0],)\n",
    "    padding = False\n",
    "```\n",
    "\n",
    "4. **Tokenization**: Since neural network models cannot process a string as it is, another step is needed to divide each item in the batch into its constructing elements, called tokens. Tokens can be words, characters, or even subword elements. BART uses Byte Pair Encoding (BPE), which divides the text into its subword elements.\n",
    "\n",
    "\n",
    "##### ‚öôÔ∏è **Configuration Parameters**\n",
    "\n",
    "| Parameter | Value | Impact |\n",
    "|-----------|--------|--------|\n",
    "| **framework** | `\"pt\"` | Returns PyTorch tensors |\n",
    "| **prefix** | `\"\"` (empty for BART) | No task prefix needed |\n",
    "| **truncation** | `DO_NOT_TRUNCATE` | Keeps full input (unless too long) |\n",
    "| **padding** | Dynamic | True for batches, False for single inputs | -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7187efa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\milli\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\37f520fa929c961707657b28798b30c003dd100b\\config.json\n",
      "Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\milli\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\37f520fa929c961707657b28798b30c003dd100b\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\milli\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\37f520fa929c961707657b28798b30c003dd100b\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading file tokenizer.json from cache at C:\\Users\\milli\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\37f520fa929c961707657b28798b30c003dd100b\\tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\milli\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\37f520fa929c961707657b28798b30c003dd100b\\config.json\n",
      "Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\milli\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\37f520fa929c961707657b28798b30c003dd100b\\config.json\n",
      "Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\milli\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\37f520fa929c961707657b28798b30c003dd100b\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\milli\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\snapshots\\37f520fa929c961707657b28798b30c003dd100b\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, AutoConfig\n",
    "from transformers.tokenization_utils import TruncationStrategy\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "framework = \"pt\"\n",
    "\n",
    "prefix = AutoConfig.from_pretrained(model_name).prefix\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab48089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils import TruncationStrategy\n",
    "\n",
    "def parse_and_tokenize(*args, truncation, prefix):\n",
    "    assert isinstance(args[0], str) or isinstance(args[0], list), f\" `args[0]`: {args[0]} have the wrong format. The should be either of type `str` or type `list`\"\n",
    "    \n",
    "    prefix = prefix if prefix is not None else \"\"\n",
    "\n",
    "    if isinstance(args[0], list):\n",
    "        if tokenizer.pad_token_id is None:\n",
    "            raise ValueError(\"Please make sure that the tokenizer has a pad_token_id when using a batch input\")\n",
    "        args = ([prefix + arg for arg in args[0]],)\n",
    "        padding = True\n",
    "    elif isinstance(args[0], str):\n",
    "        args = (prefix + args[0],)\n",
    "        padding = False\n",
    "\n",
    "    inputs = tokenizer(*args, padding=padding, truncation=truncation, return_tensors=framework)\n",
    "    if \"token_type_ids\" in inputs:\n",
    "        del inputs[\"token_type_ids\"]\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "\n",
    "def preprocess(inputs, truncation = TruncationStrategy.DO_NOT_TRUNCATE, prefix = \"\", **kwargs):\n",
    "    inputs = parse_and_tokenize(inputs, truncation=truncation, prefix = prefix, **kwargs)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b70d1f",
   "metadata": {},
   "source": [
    "##### üî¨ **Real Example Breakdown**\n",
    "\n",
    "From our test with the first 10 words:\n",
    "\n",
    "**Input Text**: `\"New York (CNN)When Liana Barrientos was 23 years old, she\"`\n",
    "\n",
    "**Tokenization Result**:\n",
    "```\n",
    "Tokens: ['<s>', ' New', ' York', ' (', 'CNN', ')', 'When', ' L', 'iana', ' Bar', 'rient', 'os', ' was', ' 23', ' years', ' old', ',', ' she', '</s>']\n",
    "```\n",
    "\n",
    "**Key Observations**:\n",
    "- `<s>` and `</s>` are automatically added (start/end tokens)\n",
    "- `ƒ†` represents spaces in BART tokenization (byte-pair encoding)\n",
    "- Punctuation is handled separately: `(` and `)` are distinct tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9cd63ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York (CNN)When Liana Barrientos was 23 years old, she\n",
      "['<s>', ' New', ' York', ' (', 'CNN', ')', 'When', ' L', 'iana', ' Bar', 'rient', 'os', ' was', ' 23', ' years', ' old', ',', ' she', '</s>']\n",
      "<s> New York (CNN)When Liana Barrientos was 23 years old, she</s>\n"
     ]
    }
   ],
   "source": [
    "def choose_first_k_words(text, k=10):\n",
    "    return \" \".join(text.split(' ')[:k])\n",
    "\n",
    "input_text = choose_first_k_words(sample_text)\n",
    "\n",
    "tokenizer_result = preprocess(input_text, prefix=prefix)\n",
    "\n",
    "tokens = [tokenizer._convert_id_to_token(x) for x in tokenizer_result[\"input_ids\"].flatten().tolist()]\n",
    "reconstructed_text = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "print(input_text)\n",
    "print([token.replace(\"ƒ†\", \" \") for token in tokens])\n",
    "print(reconstructed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf6193",
   "metadata": {},
   "source": [
    "##### **How Tokenizers Are Used**\n",
    "\n",
    "As we can see, BPE breaks down a single text into tokens, where each token have different ids. Later in the pipeline, these ids are used to get the **embeddings** for each of these tokens via a dense Embedding layer:\n",
    "\n",
    "If:\n",
    "\n",
    "- Vocabulary size = $V$  \n",
    "- Embedding dimension = $d$  \n",
    "- Embedding matrix $E \\in \\mathbb{R}^{V \\times d}$  \n",
    "\n",
    "Then for a token ID $i$:\n",
    "\n",
    "$$\n",
    "\\text{embedding}(i) = E[i]\n",
    "$$\n",
    "\n",
    "This is just a lookup into the $i$-th row of the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b17b16e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 27326, 11637,     2]])\n",
      "torch.Size([1, 4, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "text = \"apple pie\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs[\"input_ids\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model.model.shared(inputs[\"input_ids\"])\n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7ad66",
   "metadata": {},
   "source": [
    "##### üß© **Byte Pair Encoding (BPE)**\n",
    "\n",
    "**Byte Pair Encoding (BPE)** is a **subword tokenization algorithm** widely used in NLP models like GPT-2, RoBERTa, and BART. It works as follows:\n",
    "\n",
    "1. **Start with characters**\n",
    "   - The initial vocabulary contains all individual characters.\n",
    "\n",
    "   Example:  \n",
    "   `unhappiness` ‚Üí [ \"u\", \"n\", \"h\", \"a\", \"p\", \"p\", \"i\", \"n\", \"e\", \"s\", \"s\" ]\n",
    "\n",
    "2. **Find the most frequent pair of tokens**  \n",
    "   - Count all adjacent pairs in the corpus.  \n",
    "   - Merge the most frequent one into a new subword token.  \n",
    "   - Add it to the vocabulary.\n",
    "\n",
    "   Example:  \n",
    "   - \"h\" + \"a\" ‚Üí \"ha\"  \n",
    "   - Later: \"ha\" + \"p\" ‚Üí \"hap\"\n",
    "\n",
    "3. **Repeat until vocabulary size is reached**  \n",
    "   - Continue merging until you hit a predefined vocab size (e.g. 30k‚Äì50k tokens).  \n",
    "   - Frequent subwords become single tokens, while rare words are broken into smaller pieces.\n",
    "\n",
    "##### ‚öôÔ∏è **Formula (merge step)**\n",
    "At each step, find the pair:\n",
    "\n",
    "$$\n",
    "(a^*, b^*) = \\arg\\max_{(a,b)} \\text{freq}(a, b)\n",
    "$$\n",
    "\n",
    "where $\\text{freq}(a, b)$ = number of times token $a$ is followed by token $b$ in the corpus.\n",
    "\n",
    "For better understanding, here is a sample implementation of BPE with the following corpus: `\"unhappy\", \"happiness\", \"unhappiness\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8bfc769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"unhappy\", \"happiness\", \"unhappiness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3c80708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial tokenization: [['u', 'n', 'h', 'a', 'p', 'p', 'y', '</w>'], ['h', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's', '</w>'], ['u', 'n', 'h', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's', '</w>']]\n"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "def word_to_tokens(word):\n",
    "    return list(word) + [\"</w>\"]\n",
    "\n",
    "tokenized_corpus = [word_to_tokens(w) for w in corpus]\n",
    "print(\"Initial tokenization:\", tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "96c818c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('h', 'a'): 3, ('a', 'p'): 3, ('p', 'p'): 3, ('u', 'n'): 2, ('n', 'h'): 2, ('p', 'i'): 2, ('i', 'n'): 2, ('n', 'e'): 2, ('e', 's'): 2, ('s', 's'): 2, ('s', '</w>'): 2, ('p', 'y'): 1, ('y', '</w>'): 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Step 2\n",
    "def get_stats(tokenized_corpus):\n",
    "    pairs = Counter()\n",
    "    for tokens in tokenized_corpus:\n",
    "        for i in range(len(tokens)-1):\n",
    "            pairs[(tokens[i], tokens[i+1])] += 1\n",
    "    return pairs\n",
    "\n",
    "stats = get_stats(tokenized_corpus)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f7d8925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging: ('h', 'a')\n",
      "New tokenized corpus: [['u', 'n', 'ha', 'p', 'p', 'y', '</w>'], ['ha', 'p', 'p', 'i', 'n', 'e', 's', 's', '</w>'], ['u', 'n', 'ha', 'p', 'p', 'i', 'n', 'e', 's', 's', '</w>']]\n"
     ]
    }
   ],
   "source": [
    "def recalculate_tokens(new_token, old_tokens):\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    while i < len(old_tokens):\n",
    "        if i < len(old_tokens)-1 and ''.join([old_tokens[i], old_tokens[i+1]]) == new_token:\n",
    "            new_tokens.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(old_tokens[i])\n",
    "            i += 1\n",
    "\n",
    "    return new_tokens\n",
    "\n",
    "def merge_vocab(pair, tokenized_corpus):\n",
    "    new_token = ''.join(pair)\n",
    "    new_corpus = [recalculate_tokens(new_token, old_tokens) for old_tokens in tokenized_corpus]\n",
    "    \n",
    "    return new_corpus\n",
    "\n",
    "pair = stats.most_common(1)[0][0]\n",
    "print(\"Merging:\", pair)\n",
    "\n",
    "new_tokenized_corpus = merge_vocab(pair, tokenized_corpus)\n",
    "print(\"New tokenized corpus:\", new_tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "36a8b46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging: ('h', 'a')\n",
      "[['u', 'n', 'ha', 'p', 'p', 'y', '</w>'], ['ha', 'p', 'p', 'i', 'n', 'e', 's', 's', '</w>'], ['u', 'n', 'ha', 'p', 'p', 'i', 'n', 'e', 's', 's', '</w>']]\n",
      "Merging: ('ha', 'p')\n",
      "[['u', 'n', 'hap', 'p', 'y', '</w>'], ['hap', 'p', 'i', 'n', 'e', 's', 's', '</w>'], ['u', 'n', 'hap', 'p', 'i', 'n', 'e', 's', 's', '</w>']]\n",
      "Merging: ('hap', 'p')\n",
      "[['u', 'n', 'happ', 'y', '</w>'], ['happ', 'i', 'n', 'e', 's', 's', '</w>'], ['u', 'n', 'happ', 'i', 'n', 'e', 's', 's', '</w>']]\n",
      "Merging: ('u', 'n')\n",
      "[['un', 'happ', 'y', '</w>'], ['happ', 'i', 'n', 'e', 's', 's', '</w>'], ['un', 'happ', 'i', 'n', 'e', 's', 's', '</w>']]\n",
      "Merging: ('un', 'happ')\n",
      "[['unhapp', 'y', '</w>'], ['happ', 'i', 'n', 'e', 's', 's', '</w>'], ['unhapp', 'i', 'n', 'e', 's', 's', '</w>']]\n",
      "Merging: ('i', 'n')\n",
      "[['unhapp', 'y', '</w>'], ['happ', 'in', 'e', 's', 's', '</w>'], ['unhapp', 'in', 'e', 's', 's', '</w>']]\n",
      "Merging: ('in', 'e')\n",
      "[['unhapp', 'y', '</w>'], ['happ', 'ine', 's', 's', '</w>'], ['unhapp', 'ine', 's', 's', '</w>']]\n",
      "Merging: ('ine', 's')\n",
      "[['unhapp', 'y', '</w>'], ['happ', 'ines', 's', '</w>'], ['unhapp', 'ines', 's', '</w>']]\n",
      "Merging: ('ines', 's')\n",
      "[['unhapp', 'y', '</w>'], ['happ', 'iness', '</w>'], ['unhapp', 'iness', '</w>']]\n",
      "Merging: ('iness', '</w>')\n",
      "[['unhapp', 'y', '</w>'], ['happ', 'iness</w>'], ['unhapp', 'iness</w>']]\n"
     ]
    }
   ],
   "source": [
    "# Lets run these three steps for 10 iterations\n",
    "corpus = [\"unhappy\", \"happiness\", \"unhappiness\"]\n",
    "tokenized_corpus = [word_to_tokens(w) for w in corpus]\n",
    "\n",
    "for _ in range(10):\n",
    "    pairs = get_stats(tokenized_corpus)\n",
    "    if not pairs:\n",
    "        break\n",
    "\n",
    "    best_pair = pairs.most_common(1)[0][0]\n",
    "    \n",
    "    print(\"Merging:\", best_pair)\n",
    "    tokenized_corpus = merge_vocab(best_pair, tokenized_corpus)\n",
    "    print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f923ef",
   "metadata": {},
   "source": [
    "#### Forward\n",
    "\n",
    "When generating text (e.g., summaries), models don‚Äôt just pick the most likely token greedily.  \n",
    "Two important concepts are often used:\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Beam Search\n",
    "- Instead of keeping **only the single best sequence** at each decoding step (greedy search),  \n",
    "  beam search keeps the **top *k* candidates** (the *beam size*).\n",
    "- At each step:\n",
    "  - Expand all current candidates by one token.\n",
    "  - Keep the top *k* most probable sequences.\n",
    "- This explores more possibilities and often produces **higher-quality summaries**.\n",
    "\n",
    "Example (beam size = 3):\n",
    "- Step 1: Keep 3 most likely first tokens.\n",
    "- Step 2: Expand each, keep 3 best partial sequences.\n",
    "- Continue until end-of-sequence.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Early Stopping\n",
    "- Normally, beam search continues until **all beams reach an end token** (`</s>`).\n",
    "- With *early stopping*, decoding stops as soon as the **first beam finishes**.\n",
    "- This makes generation **faster**, but sometimes at the cost of completeness.\n",
    "- In summarization, early stopping can help avoid **overly long or repetitive outputs**.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In practice (Hugging Face `generate`):**\n",
    "- `num_beams=4` ‚Üí use beam search with 4 beams.\n",
    "- `early_stopping=True` ‚Üí stop once one beam has produced a valid ending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7ccd78e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s><s>Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. At one time, she was married to eight men at once, prosecutors say.</s>\n"
     ]
    }
   ],
   "source": [
    "def check_inputs(input_length, min_length, max_length):\n",
    "    if max_length < min_length:\n",
    "        print(f\"Your min_length={min_length} must be inferior than your max_length={max_length}.\")\n",
    "\n",
    "    if input_length < max_length:\n",
    "        print(\n",
    "            f\"Your max_length is set to {max_length}, but your input_length is only {input_length}. Since this is \"\n",
    "            \"a summarization task, where outputs shorter than the input are typically wanted, you might \"\n",
    "            f\"consider decreasing max_length manually, e.g. summarizer('...', max_length={input_length // 2})\"\n",
    "        )\n",
    "\n",
    "def forward(model, model_inputs, **generate_kwargs):\n",
    "    in_b, input_length = model_inputs[\"input_ids\"].shape\n",
    "\n",
    "    check_inputs(\n",
    "        input_length,\n",
    "        generate_kwargs.get(\"min_length\", 25),\n",
    "        generate_kwargs.get(\"max_length\", 60),\n",
    "    )\n",
    "\n",
    "    output_ids = model.generate(**model_inputs, **generate_kwargs)\n",
    "    out_b = output_ids.shape[0]\n",
    "\n",
    "    output_ids = output_ids.reshape(in_b, out_b // in_b, *output_ids.shape[1:])\n",
    "    return {\"output_ids\": output_ids}\n",
    "\n",
    "\n",
    "model_inputs = preprocess(sample_text, prefix=prefix)\n",
    "generate_kwargs = {\n",
    "    \"num_beams\": model.config.num_beams,\n",
    "    \"max_length\": model.config.max_length,\n",
    "    \"min_length\": model.config.min_length,\n",
    "    \"early_stopping\": model.config.early_stopping\n",
    "}\n",
    "\n",
    "model_outputs = forward(model, model_inputs, **generate_kwargs)\n",
    "\n",
    "tokens = [tokenizer._convert_id_to_token(x) for x in model_outputs[\"output_ids\"].flatten().cpu().tolist()]\n",
    "summary_text = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ef839",
   "metadata": {},
   "source": [
    "#### Postprocessing\n",
    "\n",
    "Postprocessing allows us to change the format of the output from a list of tokens to a clean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c73ac2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. At one time, she was married to eight men at once, prosecutors say.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def postprocess(model_outputs, clean_up_tokenization_spaces=False, return_name=\"summary\"):\n",
    "    records = []\n",
    "    for output_ids in model_outputs[\"output_ids\"][0]:\n",
    "        record = {\n",
    "            f\"{return_name}_text\": tokenizer.decode(\n",
    "                output_ids,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
    "            )\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "    return records\n",
    "\n",
    "output = postprocess(model_outputs)\n",
    "output[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d29ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22c012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084cd36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051102c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
